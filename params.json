{"name":"Byrtop10","tagline":"我的微信公共账号北邮人论坛历史10大","body":"\r\nbyrTOP10介绍 byrTOP10是一个Java Servlet 和 python Scrapy(0.24) 结合的一个微信公众帐号开发应用。功能是实现对北邮人论坛每日10大帖子的爬取后存入mysql数据库供微信公众号的检索。如输入20150831则会返回2015年8月31号的当日十大帖子。由于开发不久，数据库里的数据不多，记录日期重20150831开始，后续的数据库信息由scrapy和crontab每日定时6点30分开始爬去当日十大。\r\n\r\n![](http://i.imgur.com/nZNgLgf.png)\r\n\r\n![](http://i.imgur.com/rDJHDlT.jpg)\r\n\r\n修改mysql数据库配置\r\n```\r\nnano /etc/mysql/my.cnf \r\n#bind-address 127.0.0.1 \r\n```\r\n```\r\nsudo /etc/init.d/mysql restart #重启mysql\r\n```\r\n\r\n创建用户\r\n```\r\nCREATE USER '****'@'localhost' IDENTIFIED BY '****';\r\nCREATE USER '****'@'%' IDENTIFIED BY '****';\r\nGRANT ALL ON *.* TO '****'@'localhost';\r\nGRANT ALL ON *.* TO '****'@'%';\r\n```\r\n创建数据库\r\n```\r\nCREATE DATABASE `TOP10` CHARACTER SET utf8 COLLATE utf8_general_ci;\r\n```\r\n建top10表\r\n```\r\npython dataScrapy/t1/spiders/sp2.py\r\n```\r\n运行爬虫\r\n```\r\ncd dataScrapy\r\nscrapy crawl t2\r\n```\r\n\r\n#修改dataScrapy项目\r\n修改sp2.py 中的数据库用户名和密码\r\n#修改myJavaWeb项目\r\n修改jdbcUtils.java 中的数据库用户名和密码\r\n修改SignUtil.java 中的token\r\n\r\n#部署服务器\r\n```\r\n#修改apache-tomcat-7.0.57/conf/tomcat-users.xml \r\n<tomcat-users>\r\n  <role rolename=\"manager-gui\"/>\r\n    <role rolename=\"manager-script\"/>\r\n    <user username=\"tomcat\" password=\"tomcat\" roles=\"manager-gui, manager-script\"/>\r\n</tomcat-users>\r\n```\r\n\r\n```\r\n#修改maven settings.xml\r\n<servers>\r\n  <server>\r\n    <id>Tomcat7</id>\r\n    <username>tomcat</username>\r\n    <password>tomcat</password>\r\n  </server>\r\n</servers>\r\n```\r\n\r\n```\r\ncd myJavaWeb\r\n#运行tomcat7\r\nmvn tomcat7:deploy\r\n```\r\n\r\n#增加爬虫定时任务 crontab\r\n\r\n>由于是美国的VPS，所以改下时区\r\n\r\n```\r\nsudo dpkg-reconfigure tzdata #选取Asia Shanghai就可以了\r\n```\r\n\r\n```\r\nnano Top10.sh\r\n\r\n#!/bin/bash\r\nexport PATH=\"/usr/local/bin:/usr/bin:/bin\"\r\ncd `dirname $0`\r\nexec \"$@\"\r\n\r\n\r\n```\r\n\r\n```\r\ncrontab -e \r\n```\r\n```\r\n30 6 * * * /root/tomcat/dataScrapy/Top10.sh  scrapy crawl t2   #每天6点30分去爬论坛数据\r\n```","google":"UA-56540551-2","note":"Don't delete this file! It's used internally to help with page regeneration."}